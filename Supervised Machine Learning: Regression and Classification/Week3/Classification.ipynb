{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62eef828-36c7-4cba-b989-d3020189e0e0",
   "metadata": {},
   "source": [
    "## In This Week we will discuss about Classification problems \n",
    "**1. Classification is used when the output variable can take on only a small number of possible values. Examples of classification problems include spam email detection, fraudulent transaction identification, and tumor classification. The instructor explains that linear regression, which is used for predicting numbers, is not suitable for classification problems. Instead, logistic regression is introduced as a popular algorithm for classification. The instructor also discusses the use of thresholds in classification and the importance of the decision boundary. Linear regression is shown to be inadequate for classification tasks, leading to the need for specialized algorithms like logistic regression.**\n",
    "<img src=\"Images/Classification.png\" width=\"800\"/>\n",
    "\n",
    "**2.Let's Visualize with a example of a tumor is malignent or not with the example given below**\n",
    "<img src=\"Images/GraphClassification.png\" width=\"800\"/>\n",
    "\n",
    "**3.Logistic regression is a widely used classification algorithm in machine learning. It is used to predict the probability of a binary outcome based on input features. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of an event occurring. The output of logistic regression is a value between 0 and 1, which represents the probability of the event happening. It uses a mathematical function called the sigmoid function to map the input features to the predicted probability. Logistic regression is commonly used in various fields, including healthcare, finance, and marketing, for tasks such as predicting disease outcomes, credit risk assessment, and customer churn prediction**\n",
    "<img src=\"Images/logisticregression.png\" width=\"800\"/>\n",
    "\n",
    "**4.Formula to derive Logistic Regression from Linear Regression**\n",
    "    <img src=\"Images/FormulaLogReg.png\" width=\"800\"/>\n",
    "\n",
    "**5.Representation of Logistic Regression**\n",
    "<img src=\"Images/InterpretationofLogisticRegressionOutput.png\" width=\"800\"/>\n",
    "\n",
    "**Note: Difference between Logistic Regression and Sigmoid Function**\n",
    "The sigmoid function is not the same as logistic regression, but it is a crucial component of logistic regression. Logistic regression is a classification algorithm that uses the sigmoid function to model the relationship between input features and the probability of a specific outcome. The sigmoid function is an S-shaped curve that maps real-valued numbers to values between 0 and 1. It transforms the linear combination of input features into a probability value. In summary, the sigmoid function is used in logistic regression to calculate probabilities, making it an essential part of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c812f81-69f0-4293-9c6f-8702180c943c",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "**The decision boundary is a line or boundary that separates different classes in a classification problem. It is determined by the model's parameters and is used to make predictions. The decision boundary can be linear or non-linear and is based on the features of the data points. Its goal is to accurately classify data points into their respective classes.**\n",
    "\n",
    "<img src=\"Images/Decisionboundary.png\" width=\"800\"/>\n",
    "\n",
    "**2.Now Let's see the decision boundary with example below**\n",
    "\n",
    "This is a example of decision boundary for linear regression. where we can set up a boudary where our most feasible prediction lies.\n",
    "<img src=\"Images/DBeg.png\" width=\"800\"/>\n",
    "\n",
    "**Non linear decision boundary**\n",
    "\n",
    "<img src=\"Images/nonlindb.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90170037-5162-4013-a969-767a3846b643",
   "metadata": {},
   "source": [
    "## Cost Function for Logistic Regression\n",
    "\n",
    "<img src=\"Images/CFforlinreg.png\" width=\"800\" />\n",
    "\n",
    "**Now how to choose w and b?** \n",
    "\n",
    "**Our main Concerned is to choose w and b**\n",
    "\n",
    "## This is the example of Cost Function \n",
    "<img src=\"Images/SQErrorCost.png\" width=\"800\" />\n",
    "\n",
    "## This is the example of Logistic Loss Function in mathematical representation\n",
    "<img src=\"Images/LogisticLossFunction.png\" width=\"800\"/>\n",
    "\n",
    "## The loss function which is -logarithmic is choosen to minimize the cost function \n",
    "<img src=\"Images/costfun.png\" width=\"800\" />\n",
    "\n",
    "## Let's see the another mathematical representation for Simplified Loss function\n",
    "<img src=\"Images/SimplifiedLossFunction.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48246c01-464a-4a88-b90b-f785d7adc467",
   "metadata": {},
   "source": [
    "**In the context of logistic regression, the implementation of gradient descent involves finding the optimal values for the parameters (weights and bias) of the logistic regression model. Here's a step-by-step explanation of how gradient descent is implemented:**\n",
    "\n",
    "**1.Define the cost function.**\n",
    "\n",
    "**2.Initialize the parameters (weights and bias).**\n",
    "\n",
    "**3.Calculate the gradient (partial derivatives of the cost function).**\n",
    "\n",
    "**4.Update the parameters using the gradient and a learning rate.**\n",
    "\n",
    "**5.Repeat steps 3 and 4 until convergence.**\n",
    "\n",
    "**6.Check for convergence by monitoring the decrease in the cost function.**\n",
    "\n",
    "**7.Use the trained model for making predictions.**\n",
    "\n",
    "**Let's see the gradient Descent formula and representation**\n",
    "\n",
    "Similarly as we studied in linear regresssion first find the derivative then update weights and bias  until the convergence\n",
    "<img src=\"Images/gradientDescent.png\" width=\"800\"/>\n",
    "\n",
    "## Note: The algorithm looks same as linear regression it differs due to the use of sigmod function to calculate cost ie f(w,b) uses sigmoid function ie 1/1+e^-z where z is f(z) in w.x+b if confused look at formula and analyze\n",
    "<img src=\"Images/graddesforlogreg.png\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8695d9-9497-49f8-b980-210a889e14a2",
   "metadata": {},
   "source": [
    "## Regularization and Reduce Overfitting\n",
    "**This is the example to understand how our model may underfit or be bais and using other mathematical fomulation it can be sometimes preety well and even overfit the model.**\n",
    "    \n",
    "<img src=\"Images/Overfitting.png\" width=\"800\"/>\n",
    "\n",
    "**This is the different Classification example where our model underfit,pretty well and overfit.**\n",
    "<img src=\"Images/Classificationfittingmodel.png\" width=\"800\"/>\n",
    "\n",
    "**Address Overfitting**\n",
    "There are three ways to address overfitting in machine learning models: collect more data, use fewer features, and apply regularization.\n",
    "<img src=\"Images/AddressOverfitting.png\" width=\"800\"/>\n",
    "\n",
    "**Feature Selection**\n",
    "\n",
    "<img src=\"Images/Featuresel.png\" width=\"800\"/>\n",
    "\n",
    "**Regularization**\n",
    "\n",
    "<img src=\"Images/Regularization.png\" width=\"800\"/>\n",
    "\n",
    "**All three ways to reduce Overfitting**\n",
    "    \n",
    "<img src=\"Images/AddressingOverfitting.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb9a68-991c-4c9a-8340-8d949ae8556f",
   "metadata": {},
   "source": [
    "## Intution behind Regularization by comparing without regularization\n",
    "<img src=\"Images/intutionreg.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "## General formula including regularization for the model it find out pefect fit for the model neither overfits nor underfits but for that lamba should be choosen perfectly\n",
    "\n",
    "\n",
    "<img src=\"Images/reggenformula.png\" width=\"800\"/>\n",
    "\n",
    "## The way to choose lamba is that neither it should be very small value nor very large value .\n",
    "**1. If lambda is very small then it overfits the model**\n",
    "\n",
    "**2.If lambda is very large then it underfits the model**\n",
    "\n",
    "<img src=\"Images/findlambda.png\" width=\"800\" align=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557d11c-362e-4b4e-9fde-926f867f314c",
   "metadata": {},
   "source": [
    "## 1.  Let's find out the mathematical update on linear regression when we use regularization to its parameters .\n",
    "<img src=\"Images/reglinreg.png\" width=\"800\"/>\n",
    "\n",
    "## 2.  Utilizing regularization in gradient descent by simplifying the equation \n",
    "<img src=\"Images/Implementinggraddesc.png\" width=\"800\"/>\n",
    "\n",
    "## 3. Let's find out for regularized logisitic regression \n",
    "<img src=\"Images/regularizedlogreg.png\" width=\"800\"/>\n",
    "\n",
    "## 4. Now let's see for the gradient descent for logistic regression.\n",
    "<img src=\"Images/graddescforlogreg.png\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
