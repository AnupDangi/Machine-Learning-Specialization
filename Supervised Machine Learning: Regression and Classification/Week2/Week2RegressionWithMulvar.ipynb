{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32f6aa7-a280-4aef-86e2-4c250a69ec6d",
   "metadata": {},
   "source": [
    "# Week 2 \n",
    "# Regression with Multiple Variable \n",
    "In this Week we will be working with multiple variable regression model.\n",
    "\n",
    "<img src=\"images/Intro.png\" width=\"800\"/>\n",
    "\n",
    "# This is the figure representing the table with multiple variable affecting the change in output of the model .\n",
    "x1,x2,x3 and x4 are the multiple variables affecting the ouput ie price of the house.\n",
    "<img src=\"images/mulvar.png\" width=\"800\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df456f07-1523-4127-b3ff-1861af4c414c",
   "metadata": {},
   "source": [
    "# Mathematical representation for linear regression of multiple variables \n",
    "<img src=\"images/mathsmultivar.png\" width=\"800\"/>\n",
    "\n",
    "#  Here w1-> may represent the size of room increased by 0.1*x1 in feet or metre  based on input x1\n",
    "\n",
    "# Here w2->may represent the bedrooms increases by 4*x2 based on input x2\n",
    "\n",
    "# Here w3->may represent the floors increases by 10*x3 based on input x3\n",
    "\n",
    "# Here w4->may represent the cost of house depreciate by 2 on every x4 years which depends on x4 \n",
    "\n",
    "# b->may refer the base cost  which is initially given as 80.\n",
    "\n",
    "\n",
    "# Here the mathematical formulation to calculate multiple linear regression is given below\n",
    "<img src=\"images/mulvarlinreg.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108e92d-0422-4d8e-945b-a16b23841ebb",
   "metadata": {},
   "source": [
    "# Now let's talk about Vectorization\n",
    "# What is Vectorization ??\n",
    "Vectorization in the context of computing refers to the process of converting an algorithm from operating on a single value at a time (scalar operations) to operating on a set of values (vector operations) simultaneously.\n",
    "\n",
    "<img src=\"images/Vectorization.png\" width=\"800\"/>\n",
    "\n",
    "# What we achieved after this vectorization??\n",
    "\n",
    "# The mundane and repetition of tasks or calculation is not required and it is efficient than manually calculating y for multiple variable \n",
    "# If we have multiple number of variables we can't calculate manually even though we use loop it is not efficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3601c-4108-4d1c-918a-3ca638c4fa71",
   "metadata": {},
   "source": [
    "\n",
    "# 1. See the difference between using a for loop and vectorization \n",
    "<img src=\"images/compare.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "# 2. Now How does Vectorization is used in Gradient Descent\n",
    "<img src=\"images/GradientDescent.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "# 3. Now let's see the difference between previous Notations and Vectorization Notations which makes the code effiecent and works perfectly for multivariable Linear Regression.\n",
    "<img src=\"images/Notations.png\" width=\"800\"/>\n",
    "\n",
    "# 4.  Now let's see the mathematical representation of GradientDescent for Multiple Variable\n",
    "<img src=\"images/graddesmulvar.png\" width=\"800\"/>\n",
    "\n",
    "# 5.  There is an alternative to find the Gradient Descent but with some of the cost</li></ul>\n",
    "\n",
    "<img src=\"images/normaleq.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f2511-8210-42ab-bb1f-87e40321e058",
   "metadata": {},
   "source": [
    "# How to Scale the features\n",
    "# 1. Given below we can clearly see that we can scale and modify the features to get the excepted result or predict more efficiently\n",
    "<img src=\"images/FeatureScaling.png\" width=\"800\"/>\n",
    "\n",
    "# 2. While dealing with feature scaling and working with multiple variable which affects the gradient descent slowly modify the gradient descent managing all the weights\n",
    ". From this we can conclude that managing the values of w and b can be helpful to make the gradient descent work effiectly and provide minimum error. \n",
    ".From this i can say that weights and bais should be properly utilized and managed.\n",
    "<img src=\"images/modifyfeatval.png\" width=\"800\"/>\n",
    "\n",
    "# 3. While managing the range of the weights we can change the range by dividing with maximum \n",
    "This technique work by dividing the weights by maximum weight and the range is managed within that range. As given in the figure below\n",
    "<img src=\"images/scaling.png\" width=\"800\"/>\n",
    "\n",
    "# 4. Mean normalization technique\n",
    "This technique works by finding out mean normalization technique as given below.First find the average(miu) then find the ratio by divding x-miu/max-min.\n",
    "<img src=\"images/meannormalization.png\" width=\"800\"/>\n",
    "\n",
    "# 5. Z-Score Normalization Technique which requires standard deviation\n",
    "In this method find the standard deviation and use the formula as x1=x1-miu/S.D\n",
    "<img src=\"images/Zscorenormalization.png\" width=\"800\"/>\n",
    "\n",
    "# 6. Tips to remember which Scaling the features\n",
    "\n",
    "# Rescale when the range of the values are more or very less which lies in the range.\n",
    "# Do not need to rescale when the values don't lie within the range.\n",
    "<img src=\"images/featureScaling.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405b2de-0c00-4123-b17d-2a5e3367de54",
   "metadata": {},
   "source": [
    "# Checking Gradient descent for Convergence \n",
    "# What is Convergence??\n",
    " Gradient descent is working well and converging. Gradient descent is an algorithm used in machine learning to find the best values for the parameters of a model. The goal is to minimize a cost function, which measures how well the model is performing.\n",
    "\n",
    "# Convergence refers to the point where an algorithm or process has reached a stable and optimal solution. It means that the algorithm has done its job and found the best possible outcome.\n",
    "\n",
    "The work of the gradient descent is to minimize the cost function at which w and b is minimum.\n",
    "<img src=\"images/Convergence.png\" width=\"800\"/>\n",
    "# Remember after each iteration the cost function J(w,b) should decrease if the gradient descent is working perfectly.\n",
    "# In the given figure it shows that after 100 iterations the J(w,b) is decreasing  \n",
    "# When the curve is no longer decreasing we can say that gradient descent is converging.\n",
    "# It also differs on application.\n",
    "# How to know the we reached convergence point when the cost function is bare minimum we can setup a bare minimum value by which cost function is decreasing and \n",
    "# then we can conclude that if it is less than \"Epsilon\" ie bare minimum we reached the convergence point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ea931-23f5-495e-8f37-32ced5ea42df",
   "metadata": {},
   "source": [
    "# How to Choose an appropriate Learning Rate??\n",
    "# Identify problem with gradient descent\n",
    "**The learning rate is a hyperparameter in machine learning algorithms that determines the step size at which the model updates its parameters during the training process. It controls how quickly or slowly the model learns from the training data.**\n",
    "\n",
    "<img src=\"images/learningrate.png\" width=\"800\"/>\n",
    "\n",
    "1. **Appropriate Choice of Learning Rate:**\n",
    "\n",
    "The learning algorithm performs better with an appropriate choice of learning rate.\n",
    "\n",
    "2. **Small Learning Rate:**\n",
    "\n",
    "If the learning rate is too small, the algorithm runs slowly.\n",
    "\n",
    "3. **Large Learning Rate:**\n",
    "\n",
    "If the learning rate is too large, it may not converge.\n",
    "\n",
    "4. **Cost Function Fluctuations:**\n",
    "\n",
    "If the cost function sometimes goes up and sometimes goes down, it indicates that gradient descent is not working properly.\n",
    "\n",
    "5. **Overshooting the Minimum:**\n",
    "\n",
    "A large learning rate can cause the update step to overshoot the minimum, leading to an increase in cost.\n",
    "\n",
    "6. **Smaller Learning Rate:**\n",
    "\n",
    "Choosing a smaller learning rate can fix the issue and help the cost consistently decrease until it reaches the global minimum.\n",
    "\n",
    "7. **Parameter Update with Minus Sign:**\n",
    "\n",
    "It is important to use the minus sign in the code for updating the parameter to ensure the cost decreases on every iteration.\n",
    "\n",
    "8. **Set Such values of alpha or learning rate so that your model neither overfits or underfits the values.**\n",
    "<img src=\"images/tryalpha.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "**Summary Note: Set the learning curve alpha such that it is neither very small nor very large which doesnot lead to the convergence of gradient descent.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b3296-5f35-4424-8d7c-4462dffeaa95",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "**Definition: IT refers to the analytical observation of the problem to withdraw and get any new feature through which the new feature can be extracted.**\n",
    "From the figure below we can observe the new feature of the house and find out its area rather than manually using other formula to minimize cost function.\n",
    "<img src=\"images/featureengineering.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea355eb-d23f-4d6a-a860-b9dbee68c302",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "**Definition: Polynomial regression is a type of regression analysis where the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth degree polynomial. In simpler terms, it allows us to fit curves or non-linear functions to our data.**\n",
    "\n",
    "Here's a real-life example to help you understand polynomial regression better:\n",
    "\n",
    "Let's say you are a real estate agent and you want to predict the price of houses based on their size. You collect data on various houses, including their size in square feet and their corresponding prices. When you plot this data on a graph, you notice that a straight line doesn't fit the data very well. Instead, you want to fit a curve to the data to get a better prediction.\n",
    "In this case, you can use polynomial regression to fit a curve to the data. You can choose to use a quadratic function, which includes the size (x) and the size squared (x^2) as features. This quadratic model may give you a better fit to the data because it captures the non-linear relationship between the size of the house and its price.\n",
    "\n",
    "By using polynomial regression, you can explore different degrees of polynomials (e.g., quadratic, cubic, etc.) and choose the one that best fits your data. This allows you to capture more complex relationships between variables and make more accurate predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
